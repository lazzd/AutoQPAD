{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazzaro/miniconda3/envs/exp_hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from auto_gptq.utils.peft_utils import GPTQLoraConfig\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, get_gptq_peft_model, BaseQuantizeConfig\n",
    "\n",
    "from peft.tuners.lora import LoraLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTR\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "How ft Llama2 model, with PAD token.\"\"\")\n",
    "parser.add_argument('--save_emb', type=bool, default=True,\n",
    "                    help=\"Save the embedding tokens layer with LoRA.\")\n",
    "parser.add_argument('--save_model_config', type=bool, default=True,\n",
    "                    help=\"Save the config of the model.\")         \n",
    "parser.add_argument('--save_tokenizer', type=bool, default=True,\n",
    "                    help=\"Save the tokenizer.\")\n",
    "parser.add_argument('--fp16', type=bool, default=False,\n",
    "                    help=\"fp16.\")\n",
    "parser.add_argument('--bf16', type=bool, default=False,\n",
    "                    help=\"bf16.\")\n",
    "parser.add_argument('--max_memory_MB', type=int, default=24000,\n",
    "                    help=\"Free memory per gpu.\")\n",
    "parser.add_argument('--gradient_checkpointing', type=bool, default=True,\n",
    "                    help=\"Use gradient checkpointing. You want to use this.\")\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help=\"Seed.\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHERS\n",
    "\n",
    "# ------------------\n",
    "\n",
    "save_path = './ft_llama2'\n",
    "\n",
    "# ------------------\n",
    "\n",
    "CHECK = True\n",
    "\n",
    "# ------------------\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "\n",
    "# ------------------\n",
    "\n",
    "DATA_TRAIN_LENGTH = 1000\n",
    "\n",
    "# ------------------\n",
    "\n",
    "R_LORA = 64\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# ------------------\n",
    "\n",
    "CXT_LENGHT = 256\n",
    "\n",
    "# ------------------\n",
    "\n",
    "MICRO_BATCH_SIZE = 32\n",
    "BATCH_SIZE = 512\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "EPOCHS = 1\n",
    "# LEARNING_RATE = 3e-5\n",
    "LEARNING_RATE = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch` and/or `tf` (if installed).\n",
    "\n",
    "    Args:\n",
    "        seed (`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # ^^ safe to call this function even if cuda is not available\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /home/lazzaro/.cache/huggingface/hub/models--TheBloke--Luna-AI-Llama2-Uncensored-GPTQ/snapshots/67eb2ec5cad2c73dd7cb63e4acab78f7acae164c/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "\n",
    "# ------------------\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "# ------------------\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{args.max_memory_MB}MB'\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model_name_or_path = \"TheBloke/Luna-AI-Llama2-Uncensored-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=False,\n",
    "    device_map='auto',\n",
    "    # max_memory=max_memory,\n",
    "    inject_fused_attention = True,\n",
    "    inject_fused_mlp = False,\n",
    "    use_triton=True,\n",
    "    warmup_triton=False,\n",
    "    trainable=True,\n",
    "    quantize_config=quantize_config # for BaseQuantizeConfig from pretrained\n",
    ")\n",
    "\n",
    "## otherwise, set quantize_config in this way\n",
    "# model.model.quantize_config = model.quantize_config\n",
    "\n",
    "model.train()\n",
    "\n",
    "model.config.torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaTokenizerFast\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token == '<unk>':\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )\n",
    "    \n",
    "if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "    # LLaMA tokenizer may not have correct special tokens set.\n",
    "    # Check and add them if missing to prevent them from being parsed into different tokens.\n",
    "    # Note that these are present in the vocabulary. \n",
    "    # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "            \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "            \"unk_token\": tokenizer.convert_ids_to_tokens(model.config.pad_token_id),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 405,282,816 || all params: 1,347,485,696 || trainable%: 30.07696610087058\n"
     ]
    }
   ],
   "source": [
    "if args.save_emb:\n",
    "    # target_modules: default all linear layers of att, in our case (llama2): \"up_proj\", \"down_proj\", \"qkv_proj\", \"gate_proj\", \"o_proj\"\n",
    "\n",
    "    config = GPTQLoraConfig(\n",
    "        r=R_LORA,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\",\n",
    "        modules_to_save=['embed_tokens']\n",
    "    )\n",
    "else:\n",
    "    config = GPTQLoraConfig(\n",
    "        r=R_LORA,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "model = get_gptq_peft_model(model, config, auto_find_all_linears=True, train_mode=True)\n",
    "\n",
    "if args.gradient_checkpointing:\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoraLayer):\n",
    "        if args.bf16:\n",
    "            module = module.to(torch.bfloat16)\n",
    "    if 'norm' in name:\n",
    "        module = module.to(torch.float32)\n",
    "    if 'lm_head' in name or 'embed_tokens' in name:\n",
    "        if hasattr(module, 'weight'):\n",
    "            if args.bf16 and module.weight.dtype == torch.float32:\n",
    "                module = module.to(torch.bfloat16)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 2.3651e-03, -3.4142e-03,  9.9373e-04,  ..., -8.8348e-03,\n",
      "          2.5311e-03, -3.8948e-03],\n",
      "        [ 1.0674e-02,  1.0468e-02, -5.1956e-03,  ...,  2.9011e-03,\n",
      "          6.0844e-04, -4.6196e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [ 7.0462e-04,  6.7896e-04, -4.5277e-04,  ..., -4.3376e-04,\n",
      "         -8.9103e-05, -4.8357e-04]], device='cuda:0')\n",
      "SAVED\n",
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 2.3651e-03, -3.4142e-03,  9.9373e-04,  ..., -8.8348e-03,\n",
      "          2.5311e-03, -3.8948e-03],\n",
      "        [ 1.0674e-02,  1.0468e-02, -5.1956e-03,  ...,  2.9011e-03,\n",
      "          6.0844e-04, -4.6196e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [ 7.0462e-04,  6.7896e-04, -4.5277e-04,  ..., -4.3376e-04,\n",
      "         -8.9103e-05, -4.8357e-04]], device='cuda:0')\n",
      "LORA A\n",
      "torch.Size([64, 4096])\n",
      "tensor([[-0.0155,  0.0013, -0.0189,  ..., -0.0250,  0.0003,  0.0134],\n",
      "        [ 0.0237, -0.0229, -0.0221,  ..., -0.0078,  0.0160,  0.0318],\n",
      "        [-0.0100,  0.0070, -0.0025,  ..., -0.0263,  0.0356, -0.0043],\n",
      "        ...,\n",
      "        [ 0.0023,  0.0132,  0.0297,  ...,  0.0041,  0.0279,  0.0204],\n",
      "        [-0.0073,  0.0239, -0.0163,  ...,  0.0004,  0.0162,  0.0339],\n",
      "        [ 0.0030, -0.0131,  0.0263,  ...,  0.0046, -0.0201,  0.0198]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if CHECK:\n",
    "    if args.save_emb:\n",
    "        print('ORIGINAL')\n",
    "        print(model.model.model.embed_tokens.original_module.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.original_module.weight.data)\n",
    "        print('SAVED')\n",
    "        print(model.model.model.embed_tokens.modules_to_save.default.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.modules_to_save.default.weight.data)\n",
    "    else:\n",
    "        print(model.model.model.embed_tokens.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.weight.data)\n",
    "    \n",
    "    print('LORA A')\n",
    "    print(model.model.model.layers[0].self_attn.qkv_proj.lora_A.default.weight.data.shape)\n",
    "    print(model.model.model.layers[0].self_attn.qkv_proj.lora_A.default.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 6734.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-it\")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def template_prompt(eos_token):\n",
    "\n",
    "    template_input_en_it = \"\"\"Traduci il testo dall'inglese all'italiano.\n",
    "\n",
    "    EN: {query}\n",
    "\n",
    "    IT: {answer}{eos_token}'\"\"\"\n",
    "\n",
    "    prompt_template_en_it = PromptTemplate(\n",
    "        input_variables=[\"query\", \"answer\", \"eos_token\"],\n",
    "        template=template_input_en_it\n",
    "    )\n",
    "\n",
    "    template_input_it_en = \"\"\"Translate the text from italian to english.\n",
    "\n",
    "    IT: {query}\n",
    "\n",
    "    EN: {answer}{eos_token}\"\"\"\n",
    "\n",
    "    prompt_template_it_en = PromptTemplate(\n",
    "        input_variables=[\"query\", \"answer\", \"eos_token\"],\n",
    "        template=template_input_it_en\n",
    "    )\n",
    "\n",
    "    def generate_prompt(data):\n",
    "        \n",
    "        rand = random.choice([0, 1])\n",
    "\n",
    "        if(rand == 0):\n",
    "            return prompt_template_en_it.format(query=data[\"translation\"]['en'], answer=data[\"translation\"]['it'], eos_token=eos_token)\n",
    "        else:\n",
    "            return prompt_template_it_en.format(query=data[\"translation\"]['it'], answer=data[\"translation\"]['en'], eos_token=eos_token)\n",
    "    \n",
    "    return generate_prompt\n",
    "\n",
    "generate_prompt_funct = template_prompt(tokenizer.eos_token)\n",
    "\n",
    "def tok_function(prompt):\n",
    "\n",
    "    # print(prompt)\n",
    "\n",
    "    return tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=CXT_LENGHT + 1,\n",
    "            # return_overflowing_tokens=True,\n",
    "            # return_length=True,\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "data_train = dataset['train'].shuffle().select(range(DATA_TRAIN_LENGTH)).map(lambda data: tok_function(generate_prompt_funct(data)), remove_columns=dataset['train'].column_names)\n",
    "data_eval = dataset['validation'].shuffle().map(lambda data: tok_function(generate_prompt_funct(data)), remove_columns=dataset['validation'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_eval,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=0,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"FIX_pad_trans_outputs\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_total_limit=3,\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=1\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 50%|█████     | 1/2 [03:37<03:37, 217.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8979, 'learning_rate': 0.0015, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [06:54<00:00, 207.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1942, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 414.2142, 'train_samples_per_second': 2.414, 'train_steps_per_second': 0.005, 'train_loss': 2.5460572242736816, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=2.5460572242736816, metrics={'train_runtime': 414.2142, 'train_samples_per_second': 2.414, 'train_steps_per_second': 0.005, 'train_loss': 2.5460572242736816, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.original_module.weight torch.Size([32001, 4096])\n",
      "base_model.model.model.embed_tokens.modules_to_save.default.weight torch.Size([32001, 4096])\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight torch.Size([12288, 64])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "\n",
      "-------------------------\n",
      "\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): GPTQLoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): ModulesToSaveWrapper(\n",
      "          (original_module): Embedding(32001, 4096)\n",
      "          (modules_to_save): ModuleDict(\n",
      "            (default): Embedding(32001, 4096)\n",
      "          )\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): FusedLlamaAttentionForQuantizedModel(\n",
      "              (qkv_proj): GPTQLoraLinear(\n",
      "                in_features=4096, out_features=12288, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=12288, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (linear_module): GeneralQuantLinear(in_features=4096, out_features=12288, bias=True)\n",
      "              )\n",
      "              (o_proj): GPTQLoraLinear(\n",
      "                in_features=4096, out_features=4096, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (linear_module): GeneralQuantLinear(in_features=4096, out_features=4096, bias=True)\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (act_fn): SiLUActivation()\n",
      "              (down_proj): GPTQLoraLinear(\n",
      "                in_features=11008, out_features=4096, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (linear_module): GeneralQuantLinear(in_features=11008, out_features=4096, bias=True)\n",
      "              )\n",
      "              (gate_proj): GPTQLoraLinear(\n",
      "                in_features=4096, out_features=11008, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (linear_module): GeneralQuantLinear(in_features=4096, out_features=11008, bias=True)\n",
      "              )\n",
      "              (up_proj): GPTQLoraLinear(\n",
      "                in_features=4096, out_features=11008, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (linear_module): GeneralQuantLinear(in_features=4096, out_features=11008, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print require grad\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)\n",
    "\n",
    "print('\\n-------------------------\\n')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "\n",
    "if args.save_emb:\n",
    "    model.save_pretrained(f'{save_path}_emb_model')\n",
    "else:\n",
    "    model.save_pretrained(f'{save_path}_model')\n",
    "\n",
    "if args.save_model_config: model.config.to_json_file(f'{save_path}_model_config.json')\n",
    "\n",
    "if args.save_tokenizer: tokenizer.save_pretrained(f'{save_path}_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 2.3651e-03, -3.4142e-03,  9.9373e-04,  ..., -8.8348e-03,\n",
      "          2.5311e-03, -3.8948e-03],\n",
      "        [ 1.0674e-02,  1.0468e-02, -5.1956e-03,  ...,  2.9011e-03,\n",
      "          6.0844e-04, -4.6196e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [ 7.0462e-04,  6.7896e-04, -4.5277e-04,  ..., -4.3376e-04,\n",
      "         -8.9103e-05, -4.8357e-04]], device='cuda:0')\n",
      "SAVED\n",
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [-1.8287e-03, -5.7180e-03, -1.0537e-03,  ..., -6.0834e-03,\n",
      "          6.8041e-03, -8.3523e-03],\n",
      "        [ 6.4600e-03,  6.3234e-03, -9.1933e-04,  ..., -1.3286e-03,\n",
      "          5.1076e-03, -7.3930e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [ 7.0462e-04,  6.7896e-04, -4.5277e-04,  ..., -4.3376e-04,\n",
      "         -8.9103e-05, -4.8357e-04]], device='cuda:0')\n",
      "LORA A\n",
      "torch.Size([64, 4096])\n",
      "tensor([[-0.0166,  0.0024, -0.0178,  ..., -0.0261,  0.0014,  0.0123],\n",
      "        [ 0.0226, -0.0218, -0.0232,  ..., -0.0067,  0.0149,  0.0307],\n",
      "        [-0.0111,  0.0059, -0.0036,  ..., -0.0274,  0.0367, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0035,  0.0120,  0.0308,  ...,  0.0053,  0.0290,  0.0215],\n",
      "        [-0.0062,  0.0250, -0.0152,  ...,  0.0015,  0.0151,  0.0350],\n",
      "        [ 0.0041, -0.0120,  0.0252,  ...,  0.0056, -0.0212,  0.0187]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if CHECK:\n",
    "    if args.save_emb:\n",
    "        print('ORIGINAL')\n",
    "        print(model.model.model.embed_tokens.original_module.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.original_module.weight.data)\n",
    "        print('SAVED')\n",
    "        print(model.model.model.embed_tokens.modules_to_save.default.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.modules_to_save.default.weight.data)\n",
    "    else:\n",
    "        print(model.model.model.embed_tokens.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.weight.data)\n",
    "    \n",
    "    print('LORA A')\n",
    "    print(model.model.model.layers[0].self_attn.qkv_proj.lora_A.default.weight.data.shape)\n",
    "    print(model.model.model.layers[0].self_attn.qkv_proj.lora_A.default.weight.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
