{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazzaro/miniconda3/envs/exp_hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, LlamaTokenizerFast\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, get_gptq_peft_model, BaseQuantizeConfig\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTR\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "How load ft Llama2 model, with PAD token.\"\"\")\n",
    "parser.add_argument('--load_emb', type=bool, default=True,\n",
    "                    help=\"Load the embedding tokens layer with LoRA.\")\n",
    "parser.add_argument('--load_model_config', type=bool, default=True,\n",
    "                    help=\"Load the config of the model.\")         \n",
    "parser.add_argument('--load_tokenizer', type=bool, default=True,\n",
    "                    help=\"Save the tokenizer.\")\n",
    "parser.add_argument('--seed', type=int, default=123,\n",
    "                    help=\"Seed.\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHERS\n",
    "\n",
    "# ------------------\n",
    "\n",
    "save_path = './ft_llama2'\n",
    "\n",
    "# ------------------\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "\n",
    "# ------------------\n",
    "\n",
    "CHECK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import transformers\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch` and/or `tf` (if installed).\n",
    "\n",
    "    Args:\n",
    "        seed (`int`): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # ^^ safe to call this function even if cuda is not available\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /home/lazzaro/.cache/huggingface/hub/models--TheBloke--Luna-AI-Llama2-Uncensored-GPTQ/snapshots/67eb2ec5cad2c73dd7cb63e4acab78f7acae164c/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "\n",
    "# ------------------\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "# ------------------\n",
    "\n",
    "model_name_or_path = \"TheBloke/Luna-AI-Llama2-Uncensored-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "use_triton = True\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        # revision=\"gptq-8bit-64g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=False,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        inject_fused_mlp=False,\n",
    "        quantize_config=None)\n",
    "\n",
    "if args.load_model_config:\n",
    "    model_config = AutoConfig.from_pretrained(f'{save_path}_model_config.json')\n",
    "    model.config = model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER\n",
    "\n",
    "if args.load_tokenizer:\n",
    "    tokenizer_id = f'{save_path}_tokenizer'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_id,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "\n",
    "    # it is necessary to resizing the emb\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token == '<unk>':\n",
    "        # print('AAA')\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        # 1. Non sembra essere questo il problema...\n",
    "        # 2. Inoltre secondo me non corretto mettere l'unk_token come pad...\n",
    "        if isinstance(tokenizer, LlamaTokenizerFast):\n",
    "            # LLaMA tokenizer may not have correct special tokens set.\n",
    "            # Check and add them if missing to prevent them from being parsed into different tokens.\n",
    "            # Note that these are present in the vocabulary. \n",
    "            # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
    "            tokenizer.add_special_tokens(\n",
    "                {\n",
    "                    \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "                    \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "                    \"unk_token\": tokenizer.convert_ids_to_tokens(model.config.pad_token_id),\n",
    "                }\n",
    "            )\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 2.3651e-03, -3.4142e-03,  9.9373e-04,  ..., -8.8348e-03,\n",
      "          2.5311e-03, -3.8948e-03],\n",
      "        [ 1.0674e-02,  1.0468e-02, -5.1956e-03,  ...,  2.9011e-03,\n",
      "          6.0844e-04, -4.6196e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [-2.3209e-02,  1.5396e-02,  5.8632e-03,  ...,  2.2049e-02,\n",
      "          1.6346e-03,  1.6388e-02]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "if CHECK:\n",
    "    print(model.model.model.embed_tokens.weight.data.shape)\n",
    "    print(model.model.model.embed_tokens.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ID AND LOAD\n",
    "\n",
    "if args.load_emb:\n",
    "    model_id = f'{save_path}_emb_model'\n",
    "else:\n",
    "    model_id = f'{save_path}_model'\n",
    "\n",
    "model = get_gptq_peft_model(model, model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [ 2.3651e-03, -3.4142e-03,  9.9373e-04,  ..., -8.8348e-03,\n",
      "          2.5311e-03, -3.8948e-03],\n",
      "        [ 1.0674e-02,  1.0468e-02, -5.1956e-03,  ...,  2.9011e-03,\n",
      "          6.0844e-04, -4.6196e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [-2.3209e-02,  1.5396e-02,  5.8632e-03,  ...,  2.2049e-02,\n",
      "          1.6346e-03,  1.6388e-02]], device='cuda:0', dtype=torch.float16)\n",
      "SAVED\n",
      "torch.Size([32001, 4096])\n",
      "tensor([[ 1.2517e-06, -1.7881e-06, -4.3511e-06,  ...,  8.9407e-07,\n",
      "         -6.5565e-06,  8.9407e-07],\n",
      "        [-1.8291e-03, -5.7182e-03, -1.0538e-03,  ..., -6.0844e-03,\n",
      "          6.8054e-03, -8.3542e-03],\n",
      "        [ 6.4583e-03,  6.3248e-03, -9.1934e-04,  ..., -1.3285e-03,\n",
      "          5.1079e-03, -7.3929e-03],\n",
      "        ...,\n",
      "        [-1.0742e-02,  9.3384e-03,  1.2939e-02,  ..., -3.3203e-02,\n",
      "         -1.6357e-02,  3.3875e-03],\n",
      "        [-8.3008e-03, -4.0588e-03, -1.1063e-03,  ...,  3.4790e-03,\n",
      "         -1.2939e-02,  3.1948e-05],\n",
      "        [ 7.0477e-04,  6.7902e-04, -4.5276e-04,  ..., -4.3368e-04,\n",
      "         -8.9109e-05, -4.8351e-04]], device='cuda:0', dtype=torch.float16)\n",
      "LORA A\n",
      "torch.Size([64, 4096])\n",
      "tensor([[-0.0166,  0.0024, -0.0178,  ..., -0.0261,  0.0014,  0.0123],\n",
      "        [ 0.0226, -0.0218, -0.0232,  ..., -0.0067,  0.0149,  0.0307],\n",
      "        [-0.0111,  0.0059, -0.0036,  ..., -0.0274,  0.0367, -0.0055],\n",
      "        ...,\n",
      "        [ 0.0035,  0.0120,  0.0308,  ...,  0.0053,  0.0290,  0.0215],\n",
      "        [-0.0062,  0.0250, -0.0152,  ...,  0.0015,  0.0151,  0.0350],\n",
      "        [ 0.0041, -0.0120,  0.0252,  ...,  0.0056, -0.0212,  0.0187]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if CHECK:\n",
    "    if args.load_emb:\n",
    "        print('ORIGINAL')\n",
    "        print(model.model.model.embed_tokens.original_module.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.original_module.weight.data)\n",
    "        print('SAVED')\n",
    "        print(model.model.model.embed_tokens.modules_to_save.default.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.modules_to_save.default.weight.data)\n",
    "    else:\n",
    "        print(model.model.model.embed_tokens.weight.data.shape)\n",
    "        print(model.model.model.embed_tokens.weight.data)\n",
    "    \n",
    "    print('LORA A')\n",
    "    print(model.model.model.layers[0].self_attn.qkv_proj.lora_A.default.weight.data.shape)\n",
    "    print(model.model.model.layers[0].self_attn.qkv_proj.lora_A.default.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prove\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template_input= \"\"\"Traduci il testo dall'inglese all'italiano.\n",
    "\n",
    "EN: {query}\n",
    "\n",
    "IT:\"\"\"\n",
    "\n",
    "prompt_template_input = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template_input\n",
    ")\n",
    "\n",
    "query = \"To believe, to dare, to get their imagination unlocked for what used to be impossible.\"\n",
    "\n",
    "prompt = prompt_template_input.format(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Traduci il testo dall'inglese all'italiano.\n",
      "\n",
      "EN: To believe, to dare, to get their imagination unlocked for what used to be impossible.\n",
      "\n",
      "IT: Avere, avere, avere la testa un'idea, per credere, per sostenere, per farsi spazio per ci√≤ che era impossibile.</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.4, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
